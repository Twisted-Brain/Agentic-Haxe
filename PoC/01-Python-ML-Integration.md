<div align="center">
  <img src="../assets/logo.png" alt="Agentic Haxe Logo" width="200" height="200">
</div>

# PoC 01: Haxe Python AI Chat with OpenRouter Integration

## Overview
Demonstrates **Haxe's multi-platform power** by reusing the exact same frontend and shared components from the baseline Node.js/C++ implementation (Project 0), while running a Python backend. This proves Haxe's core value proposition: **Write Once, Deploy Everywhere**.

### Key Reuse Benefits
- âœ… **Same Frontend**: Identical JavaScript frontend from Project 0
- âœ… **Same API Models**: Shared `LlmRequest`/`LlmResponse` classes
- âœ… **Same UI Components**: Reused chat interface and styling
- âœ… **Same Configuration**: Platform-agnostic `FrontendConfig`
- âœ… **Different Backend**: Python ML service with OpenRouter integration

**Result**: 80%+ code reuse across platforms with zero frontend changes!

## Target Platforms

### Primary: VPS Deployment
- **Platform**: Linux VPS (Ubuntu/Debian)
- **Runtime**: Python 3.9+ with FastAPI/Flask
- **Deployment**: Docker containers with reverse proxy
- **Scaling**: Horizontal scaling with load balancer
- **SSL**: Let's Encrypt certificates

### Secondary: Cloud Deployment
- **Platform**: AWS EC2 / Google Cloud Compute / DigitalOcean
- **Runtime**: Managed Python environments
- **Deployment**: Container orchestration (Kubernetes/Docker Swarm)
- **Scaling**: Auto-scaling groups with health checks

## Technical Goals

### Core Features (Reused from Project 0)
- âœ… **AI Chat Interface**: Same frontend from Node.js/C++ baseline
- âœ… **Shared API Models**: Reused `ApiModels.hx` with `LlmRequest`/`LlmResponse`
- âœ… **Frontend Configuration**: Same `FrontendConfig.hx` auto-detects Python backend
- âœ… **UI Components**: Identical chat interface, styling, and user experience

### Python-Specific Features (New)
- ðŸ†• **OpenRouter Integration**: Python-specific LLM provider access
- ðŸ†• **Python ML Libraries**: NumPy, SciPy integration capabilities
- ðŸ†• **Flask/FastAPI Server**: Python web framework implementation
- ðŸ†• **Python Deployment**: Docker containers optimized for Python runtime

### Performance Targets
- < 2s response time for LLM queries
- Support for 500+ concurrent chat sessions
- 99.9% uptime with health monitoring
- Efficient memory usage and connection pooling

## Implementation Plan

### Phase 0: Reuse Existing Components (From Project 0)
```bash
# Frontend already exists and works!
ls src/frontend/
# âœ… WebAppMain.hx - Main frontend application
# âœ… FrontendConfig.hx - Platform detection and configuration
# âœ… index.html - Chat interface HTML
# âœ… webapp-styles.css - Complete styling

# Shared models already defined!
ls src/shared/
# âœ… ApiModels.hx - LlmRequest, LlmResponse, LlmModel classes
# âœ… SharedMain.hx - Shared application logic
```

### Phase 1: Haxe Python Backend Setup
```bash
# Build Python backend (reuses shared models)
haxe build-python.hxml
# Generates: bin/python/ml_service.py
# âœ… Automatically includes ApiModels from src/shared/
# âœ… Uses same LlmRequest/LlmResponse as other platforms
```

### Phase 2: OpenRouter API Integration (Reuses Shared Models)
```haxe
// src/backend/python/OpenRouterClientPython.hx
// Uses SAME LlmRequest/LlmResponse from src/shared/ApiModels.hx!
import shared.ApiModels.LlmRequest;
import shared.ApiModels.LlmResponse;

class OpenRouterClientPython {
    private var apiKey: String;
    private var baseUrl: String = "https://openrouter.ai/api/v1";
    
    public function new(apiKey: String) {
        this.apiKey = apiKey;
    }
    
    // Same API contract as Node.js/C++ versions!
    public function chatCompletion(request: LlmRequest): Promise<LlmResponse> {
        // Python-specific HTTP implementation
        return makeRequest("/chat/completions", request);
    }
}
```

### Phase 3: Chat Service Implementation (Platform-Agnostic)
```haxe
// src/backend/python/ChatServicePython.hx
// Uses SAME shared models and interfaces!
import shared.ApiModels.LlmRequest;
import shared.ApiModels.LlmResponse;

class ChatServicePython {
    private var openRouter: OpenRouterClientPython;
    private var sessions: Map<String, ChatSession>;
    
    // IDENTICAL API to Node.js/C++ versions!
    public function processMessage(sessionId: String, message: String): Promise<String> {
        var session = getOrCreateSession(sessionId);
        // Same LlmRequest constructor as other platforms
        var request = new LlmRequest(message, "openai/gpt-4");
        return openRouter.chatCompletion(request)
            .then(response -> {
                session.addMessage(message, response.content);
                return response.content; // Same LlmResponse.content field
            });
    }
}
```

### Phase 4: Web Server & API Endpoints (Same API Contract)
```haxe
// src/platform/core/python/PythonMLMain.hx
class PythonMLMain {
    public static function main() {
        // Serve SAME frontend files from src/frontend/
        setupStaticFiles("src/frontend/");
        
        // IDENTICAL API endpoints as Node.js/C++ versions
        app.post("/api/chat", handleChatRequest);
        app.get("/health", handleHealthCheck);
        
        // Frontend auto-detects Python backend via FrontendConfig
        startServer(8080);
    }
    
    // Same request/response format as other platforms!
    private static function handleChatRequest(req: HttpRequest): HttpResponse {
        var llmRequest = LlmRequest.fromJson(req.body); // Shared model!
        var response = chatService.processMessage(llmRequest);
        return HttpResponse.json(response.toJson()); // Shared serialization!
    }
}
```

## Deployment Workflows

### VPS Deployment
```yaml
# docker-compose.python.yml
version: '3.8'
services:
  haxe-python-chat:
    build: 
      context: .
      dockerfile: Dockerfile.python
    ports:
      - "8080:8080"
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - PYTHONPATH=/app
      - FLASK_ENV=production
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/ssl
    depends_on:
      - haxe-python-chat
```

### Production Deployment
```bash
#!/bin/bash
# deploy.sh
set -e

echo "Building Haxe Python AI Chat..."
haxe build-python.hxml

echo "Building Docker image..."
docker build -f Dockerfile.python -t haxe-python-chat:latest .

echo "Deploying with Docker Compose..."
docker-compose -f docker-compose.python.yml up -d

echo "Checking health..."
sleep 10
curl -f http://localhost:8080/health || exit 1

echo "Deployment successful!"
echo "Chat available at: http://localhost:8080"
```

### CI/CD Pipeline
```yaml
# .github/workflows/deploy-python.yml
name: Deploy Python AI Chat
on:
  push:
    branches: [main]
    paths: ['src/**', 'build-python.hxml', 'Dockerfile.python']

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Haxe
        uses: krdlab/setup-haxe@v1
        with:
          haxe-version: 4.3.0
      
      - name: Build Haxe to Python
        run: |
          haxelib install hxcpp
          haxe build-python.hxml
      
      - name: Run Tests
        run: |
          cd bin/python
          python -m pytest tests/ -v
      
      - name: Deploy to Production
        if: success()
        run: |
          docker build -f Dockerfile.python -t haxe-python-chat:${{ github.sha }} .
          # Deploy to your VPS/Cloud provider
```

## API Endpoints

### Chat API
```http
POST /api/chat
Content-Type: application/json

{
  "message": "Hello, how are you?",
  "model": "openai/gpt-4",
  "session_id": "user-123",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

### Response Format
```json
{
  "content": "Hello! I'm doing well, thank you for asking. How can I help you today?",
  "status": "success",
  "model": "openai/gpt-4",
  "prompt_tokens": 12,
  "completion_tokens": 18,
  "total_tokens": 30,
  "processing_time_ms": 1250
}
```

### Health Check
```http
GET /health
# Returns: {"status": "healthy", "timestamp": "2024-01-15T10:30:00Z"}
```

## Expected Benefits

### Haxe Multi-Platform Benefits
- ðŸŽ¯ **80%+ Code Reuse**: Same frontend, models, and logic across platforms
- ðŸš€ **Instant Platform Switch**: Change backend without touching frontend
- ðŸ”§ **Consistent APIs**: Identical request/response formats across platforms
- ðŸ“± **Universal Frontend**: One chat interface works with all backends
- âš¡ **Rapid Development**: Build new platforms in hours, not weeks

### Python-Specific Benefits
- **Multi-LLM Access**: OpenRouter provides access to 20+ AI models
- **ML Ecosystem**: Native access to NumPy, TensorFlow, PyTorch
- **Production Ready**: Full error handling, logging, and monitoring
- **Cost Effective**: Pay-per-use OpenRouter pricing model
- **Easy Deployment**: Python's mature deployment ecosystem

## Success Metrics

### Haxe Multi-Platform Success
- âœ… **Frontend Reuse**: Same JavaScript frontend works with Python backend
- âœ… **Model Reuse**: Shared `LlmRequest`/`LlmResponse` classes work identically
- âœ… **Config Reuse**: `FrontendConfig` auto-detects Python backend
- âœ… **Zero Frontend Changes**: No modifications needed to existing UI
- âœ… **API Compatibility**: Same endpoints and response formats

### Python Implementation Success
- âœ… Successful Haxe â†’ Python compilation
- âœ… Working OpenRouter API integration
- âœ… Chat responses < 2s average
- âœ… 99%+ uptime in production
- âœ… Automated deployment pipeline
- âœ… Comprehensive error handling

## Getting Started

### Prerequisites (Reuse Existing Setup)
```bash
# If you already have Project 0 (Node.js/C++) running:
# âœ… Haxe is already installed
# âœ… Frontend is already built
# âœ… Shared models are already defined
# âœ… Just add Python-specific dependencies!

# Install Python dependencies
pip install -r requirements.txt

# Get OpenRouter API key (same as other platforms)
export OPENROUTER_API_KEY="your-api-key-here"
```

### Quick Start (Leveraging Existing Components)
```bash
# 1. Frontend already exists from Project 0!
# No need to rebuild - it's platform-agnostic!

# 2. Build ONLY the Python backend (reuses shared models)
make build-python

# 3. Start the Python server
make run-python

# 4. Open browser to http://localhost:8080
# 5. Same chat interface, now powered by Python!
# 6. FrontendConfig automatically detects Python backend
```

### Production Deployment
```bash
# 1. Set Mac environment variables
export OPENROUTER_API_KEY=your-key
# Add to ~/.zshrc for persistence:
echo 'export OPENROUTER_API_KEY=your-key' >> ~/.zshrc

# 2. Deploy with Docker
docker-compose -f docker-compose.python.yml up -d

# 3. Setup SSL with Let's Encrypt
certbot --nginx -d yourdomain.com
```

## Next Steps

### Completed (Reused from Project 0)
1. âœ… Frontend chat interface (reused)
2. âœ… Shared API models (reused)
3. âœ… Platform configuration system (reused)
4. âœ… UI styling and components (reused)

### Python-Specific Implementation
5. âœ… Python backend compilation target
6. âœ… OpenRouter API client integration
7. âœ… Python web server and endpoints
8. âœ… Docker deployment automation

### Future Enhancements
9. ðŸ”„ Performance optimization and monitoring
10. ðŸ“‹ Python-specific ML model integration
11. ðŸ“‹ Advanced Python deployment options
12. ðŸ“‹ Benchmark against Node.js/C++ versions

### Multi-Platform Validation
- ðŸŽ¯ **Prove Haxe Value**: Same frontend works with 3+ different backends
- ðŸŽ¯ **Measure Reuse**: Document exact percentage of code reused
- ðŸŽ¯ **Performance Compare**: Benchmark Python vs Node.js vs C++
- ðŸŽ¯ **Developer Experience**: Time to add new platform (should be < 1 day)